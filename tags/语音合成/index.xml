<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>语音合成 on StarWhisper</title>
        <link>https://blog.frostmiku.com/tags/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/</link>
        <description>Recent content in 语音合成 on StarWhisper</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 21 May 2022 17:00:00 +0800</lastBuildDate><atom:link href="https://blog.frostmiku.com/tags/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>语音合成技术的发展</title>
        <link>https://blog.frostmiku.com/archives/44/</link>
        <pubDate>Sat, 21 May 2022 17:00:00 +0800</pubDate>
        
        <guid>https://blog.frostmiku.com/archives/44/</guid>
        <description>&lt;img src="https://blog.frostmiku.com/res/image-20220521162944624.png" alt="Featured image of post 语音合成技术的发展" /&gt;&lt;p&gt;文本到言语（TTS），也称为语音合成，长期以来一直是人工智能，自然语言和语音处理领域中的一个研究方向。开发TTS系统需要有关语言和人类语音生产的知识，涉及多个学科，包括语言学，声学，数字信号处理和机器学习。&lt;/p&gt;
&lt;h2 id=&#34;历史&#34;&gt;历史&lt;/h2&gt;
&lt;p&gt;人类对语音合成最早的尝试可以追溯到12世纪，18世界中叶匈牙利科学家沃尔夫冈·冯·肯佩伦（Wolfgang von Kempelen）构建了一台使用一系列波纹管，弹簧，风笛和共振盒的说话机器，以产生一些简单的单词和简短的句子。&lt;/p&gt;
&lt;p&gt;第一个基于计算机的语音合成系统出现在20世纪下半叶。早期基于计算机的语音合成方法包括关节模拟合成，共振峰合成和拼接合成。之后随着统计机器学习的发展。基于统计参数的语音合成（SPSS）方法被提出，它可以预测诸如频谱，基频和音素时长之类的参数。到了2010年左右，基于神经网络的合成方法被提出，因其较好的合成语音质量，逐渐成为语音合成领域的主流方法。&lt;/p&gt;
&lt;h2 id=&#34;关节模拟合成&#34;&gt;关节模拟合成&lt;/h2&gt;
&lt;p&gt;关节模拟合成是一个很自然的想法，其主要思想就是模拟人类器官进行发声，诸如嘴唇、舌头、共鸣腔和声带振动。理想情况下，关节模拟合成可能是合成语音的最有效方法，因为人类就是这样发声的。&lt;/p&gt;
&lt;p&gt;然而在工程实践中，这种方法存在诸多困难，例如，我们很难收集到足够多的数据对发声器官进行建模。因此该种方法的可行性较低。&lt;/p&gt;
&lt;h2 id=&#34;共振峰合成&#34;&gt;共振峰合成&lt;/h2&gt;
&lt;p&gt;共振峰合成，是对关节模拟合成模型的一种简化，它是一种source-filter模型，其基本思想是将声带的震动看作语音信号的信号源，将嘴唇、舌头等一系列共鸣腔体看作是滤波器模型，即所谓语音就是声带振动产生的波形通过一系列的滤波器后得到的时序信号。这些滤波器通常是由专业的语言学家设置制定的，系统由多个具有不同参数的加性声学模型组合发出声音。&lt;/p&gt;
&lt;p&gt;但这样的合成系统合成出来的语音具有比较强烈的机械感，另外就是滤波器的设置需要大量的专业知识，难以设置。&lt;/p&gt;
&lt;h2 id=&#34;拼接合成&#34;&gt;拼接合成&lt;/h2&gt;
&lt;p&gt;拼接合成是指将录制的真实人类语音拆解成基本发音单元（例如音节、声韵母或音素）并储存在数据库中。在合成时，系统通过给定的文本输入在数据库中查找所需发音单元的记录并将其拼接在一起生成语音。一般而言，拼接合成的TTS生成的音频具有接近原始配音演员级别的高可理解性和正宗音色。&lt;/p&gt;
&lt;p&gt;然而，使用这种方法的系统需要大量的数据以涵盖口语发音中的所有组合。另一个缺点是，这种系统合成出来的语音不太自然，几乎没有任何的情感起伏。&lt;/p&gt;
&lt;h2 id=&#34;统计参数合成&#34;&gt;统计参数合成&lt;/h2&gt;
&lt;p&gt;为了解决拼接合成的缺点，统计参数合成被提出。其基本思想是，相比于直接拼接合成语音波形，我们可以先生成产生语音所必须的声学参数，然后通过一些算法从这些声学参数中恢复出语音波形。&lt;/p&gt;
&lt;p&gt;SPSS通常由三个组件组成：文本分析模块，参数预测模块（声学模型）和声码器 Analysis/Synthesis模块（Vocoder）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.frostmiku.com/res/image-20220521162944624.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;文本分析模块作为语音合成系统的前端，起到对系统输入文本进行预处理的作用，例如，将 &lt;em&gt;“现在16：13了”&lt;/em&gt; 这句话转换为 &lt;em&gt;“现在十六点十三分了”&lt;/em&gt; 。并对文本进行分词、语义、韵律特征提取等工作。&lt;/p&gt;
&lt;p&gt;声学模型经过配对的语言特征和声学特征训练，其中声学特征包括F0(基频)，频谱或倒谱等。声码器负责将声学模型生成的声学特征还原成波形。&lt;/p&gt;
&lt;p&gt;SPSS对比上面几种TTS系统有如下好处：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;声音听上去更自然&lt;/li&gt;
&lt;li&gt;更灵活，通过修改参数可以控制合成的语音。&lt;/li&gt;
&lt;li&gt;更低的数据开销（相较于拼接合成）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但其生成的音频音质不是很好，另外依旧具有机械感，很容易被人类分辨出这段音频是合成的。&lt;/p&gt;
&lt;p&gt;2010年之后，随着深度神经网络的发展，一些高级的网络结构被应用于参数预测模块之中用以替代传统的统计模型。&lt;/p&gt;
&lt;h3 id=&#34;spss声学模型&#34;&gt;SPSS声学模型&lt;/h3&gt;
&lt;p&gt;基于HMM的SPSS的一个主要缺点是，合成语音的质量还不够好，主要有以下两个原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;声学模型的准确性不好，预测的声学特征是过度平滑的，缺乏细节。&lt;/li&gt;
&lt;li&gt;声码技术还不够好。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第一个原因主要是由于HMM缺乏建模能力，因此提出了基于DNN的SPSS，改善了合成质量。&lt;/p&gt;
&lt;p&gt;为了更好地模拟语音话语中的跨越长时上下文的效果，利用基于LSTM的循环神经网络被提出来，以更好地模拟上下文依赖性&lt;/p&gt;
&lt;p&gt;随着深度学习的发展，许多高级神经网络结构被用来预测声学特征，比如CBHG GAN啥的。&lt;/p&gt;
&lt;p&gt;相关研究者探索一种更端到端的方式，该方式利用了基于注意力的循环序列transducer模型直接从音素序列生成声学特征，可以避免在以前的基于神经网络的声学模型中所需的逐帧对齐。&lt;/p&gt;
&lt;h2 id=&#34;神经语音合成&#34;&gt;神经语音合成&lt;/h2&gt;
&lt;p&gt;随着深度学习的发展，基于神经网络的TTS被提出，它采用神经网络作为语音合成的模型骨干。 SPSS中采用了一些早期的神经模型来代替HMM进行声学建模。&lt;/p&gt;
&lt;p&gt;后来，提议 WaveNet 直接从语言特征产生波形，这可以被视为第一个现代神经TTS模型。诸如DeepVoice 1/2 之类的其他模型仍然遵循统计参数合成中的三个组件，仅仅只是使用相应的基于神经网络的模型来升级了部分组件。&lt;/p&gt;
&lt;p&gt;此外，一些端到端模型被提出（例如Tacotron 1/2 ，Deep Voice 3和FastSpeech 1/2），以简化文本分析，并直接采用文本分析模块字符/音素序列作为输入，并用梅尔谱简化声学特征。后来，开发了完全端到端的TTS系统，以直接从文本中生成波形，例如 ClariNet，FastSpeech 2s 和 Eats。与以前基于拼接合成和统计参数合成的TTS系统相比，基于神经网络的语音合成的优势包括在清晰度和自然性方面具有很高的质量，并且对人类预处理和特征构建的需求较少。&lt;/p&gt;
&lt;h3 id=&#34;端到端tts中的声学模型&#34;&gt;端到端TTS中的声学模型&lt;/h3&gt;
&lt;p&gt;与SPSS相比，基于神经网络的端到端TTS中的声学模型具有以下优点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;传统的声学模型需要在语言和声学特征之间进行对齐，而Seq2Seq的神经模型则通过注意力机制隐式地学习对齐方式或预测共同出现时间，更端到端，需要较少的预处理。&lt;/li&gt;
&lt;li&gt;受益于神经网络强大的建模能力，语义特征被简化为了仅仅只需要字符或音素序列的输入即可，同时声学特征的输出则由原本低维的倒谱上升到了更高维的线性语谱图。&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
